{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":2645886,"sourceType":"datasetVersion","datasetId":1608934}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Brain Tumor Classification","metadata":{}},{"cell_type":"markdown","source":"## Loading the Data","metadata":{}},{"cell_type":"code","source":"import kagglehub\npath = kagglehub.dataset_download(\"masoudnickparvar/brain-tumor-mri-dataset\")\n\nprint(\"Path to dataset files:\", path)","metadata":{"id":"W1jOvRpLg05C","outputId":"fd779e78-bd81-453b-d7a1-778040703721","trusted":true,"execution":{"iopub.status.busy":"2024-11-22T13:05:05.149106Z","iopub.execute_input":"2024-11-22T13:05:05.15006Z","iopub.status.idle":"2024-11-22T13:05:05.831348Z","shell.execute_reply.started":"2024-11-22T13:05:05.150022Z","shell.execute_reply":"2024-11-22T13:05:05.830522Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Installing Libraries","metadata":{}},{"cell_type":"code","source":"!pip install torchmetrics\n!pip install torchsummary\n!pip install tqdm","metadata":{"id":"e-mUtiu0tiaN","outputId":"4d936954-6d4b-4e0f-903e-93d7e8b39b37","trusted":true,"execution":{"iopub.status.busy":"2024-11-22T13:05:11.202774Z","iopub.execute_input":"2024-11-22T13:05:11.203334Z","iopub.status.idle":"2024-11-22T13:05:37.375345Z","shell.execute_reply.started":"2024-11-22T13:05:11.203304Z","shell.execute_reply":"2024-11-22T13:05:37.37425Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data Preparation","metadata":{}},{"cell_type":"code","source":"import os\nos.listdir(path)","metadata":{"id":"jkob-CgKg6s4","outputId":"529da6d1-b1ca-4a94-acab-fdaf29eaa16f","trusted":true,"execution":{"iopub.status.busy":"2024-11-22T13:05:37.377214Z","iopub.execute_input":"2024-11-22T13:05:37.377545Z","iopub.status.idle":"2024-11-22T13:05:37.38671Z","shell.execute_reply.started":"2024-11-22T13:05:37.37751Z","shell.execute_reply":"2024-11-22T13:05:37.385748Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_path = os.path.join(path, \"Testing\")\ntrain_path = os.path.join(path, \"Training\")\nclassnames = os.listdir(train_path)\nclassnames","metadata":{"id":"El5fYf93hBzG","outputId":"e141f6a6-69eb-442c-e015-12e251d7d4d1","trusted":true,"execution":{"iopub.status.busy":"2024-11-22T13:05:37.387735Z","iopub.execute_input":"2024-11-22T13:05:37.388Z","iopub.status.idle":"2024-11-22T13:05:37.396592Z","shell.execute_reply.started":"2024-11-22T13:05:37.387973Z","shell.execute_reply":"2024-11-22T13:05:37.395759Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for classname in classnames:\n    print(classname, len(os.listdir(os.path.join(test_path, classname))))","metadata":{"id":"WBUBrdOzhR46","outputId":"63eb733b-49b5-470f-af34-ec9dad623738","trusted":true,"execution":{"iopub.status.busy":"2024-11-22T13:05:43.477636Z","iopub.execute_input":"2024-11-22T13:05:43.47843Z","iopub.status.idle":"2024-11-22T13:05:43.5161Z","shell.execute_reply.started":"2024-11-22T13:05:43.478398Z","shell.execute_reply":"2024-11-22T13:05:43.51534Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for classname in classnames:\n    print(classname, len(os.listdir(os.path.join(train_path, classname))))","metadata":{"id":"uieHwO66hLJ8","outputId":"dfbe3827-b599-47ea-9044-889eaef05c98","trusted":true,"execution":{"iopub.status.busy":"2024-11-22T13:05:44.839023Z","iopub.execute_input":"2024-11-22T13:05:44.83931Z","iopub.status.idle":"2024-11-22T13:05:44.895489Z","shell.execute_reply.started":"2024-11-22T13:05:44.839285Z","shell.execute_reply":"2024-11-22T13:05:44.894647Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Creating the dataframe","metadata":{}},{"cell_type":"code","source":"# create a dataframe with image_path and image_class\nimport pandas as pd\n\ntrain_df = []\ntest_df = []\nfor classname in classnames:\n    for image in os.listdir(os.path.join(train_path, classname)):\n        image_path = os.path.join(train_path, classname, image)\n        train_df.append({'image_path': image_path, 'image_class': classname})\n\nfor classname in classnames:\n    for image in os.listdir(os.path.join(test_path, classname)):\n        image_path = os.path.join(test_path, classname, image)\n        test_df.append({'image_path': image_path, 'image_class': classname})\n\n\ntrain_df = pd.DataFrame(train_df)\ntest_df = pd.DataFrame(test_df)\ntrain_df.shape, test_df.shape","metadata":{"id":"TUfnNfqYhRCb","outputId":"1cdf7602-41f4-425a-e66d-291c78af24d7","trusted":true,"execution":{"iopub.status.busy":"2024-11-22T13:05:48.644472Z","iopub.execute_input":"2024-11-22T13:05:48.645186Z","iopub.status.idle":"2024-11-22T13:05:48.951034Z","shell.execute_reply.started":"2024-11-22T13:05:48.645152Z","shell.execute_reply":"2024-11-22T13:05:48.950184Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df = train_df[train_df['image_path'].str.endswith(('.jpg', '.png'))]\ntest_df = test_df[test_df['image_path'].str.endswith(('.jpg', '.png'))]\ntrain_df.shape, test_df.shape","metadata":{"id":"KfdoIFSrh71a","outputId":"8ce7bffe-cf8b-48cb-b4a0-9bbc5a228034","trusted":true,"execution":{"iopub.status.busy":"2024-11-22T13:05:52.753185Z","iopub.execute_input":"2024-11-22T13:05:52.753747Z","iopub.status.idle":"2024-11-22T13:05:52.767781Z","shell.execute_reply.started":"2024-11-22T13:05:52.753694Z","shell.execute_reply":"2024-11-22T13:05:52.766768Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Visualising Class distribution","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfig, axs = plt.subplots(ncols=2)\ntrain_df['image_class'].value_counts().plot(kind='pie', ax=axs[0])\ntest_df['image_class'].value_counts().plot(kind='pie', ax=axs[1])\naxs[0].set_title('Train set')\naxs[1].set_title('Test Set')\nfig.suptitle('Brain tumor class distribution')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T13:05:55.769158Z","iopub.execute_input":"2024-11-22T13:05:55.76995Z","iopub.status.idle":"2024-11-22T13:05:56.021255Z","shell.execute_reply.started":"2024-11-22T13:05:55.769915Z","shell.execute_reply":"2024-11-22T13:05:56.020191Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Encoding the class\n\nEncoding the class using label encoder from sklearn","metadata":{}},{"cell_type":"code","source":"import joblib\nfrom sklearn.preprocessing import LabelEncoder\n\n# Create a LabelEncoder object\nlabel_encoder = LabelEncoder()\n\n# Fit and transform the 'image_class' column\ntrain_df['encoded_class'] = label_encoder.fit_transform(train_df['image_class'])\ntest_df['encoded_class'] = label_encoder.transform(test_df['image_class'])\n\njoblib.dump(label_encoder, \"label_encoder.joblib\")\n\n# Print the mapping between original class names and encoded labels\nprint(dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))))","metadata":{"id":"yXgw03z_jCgU","outputId":"7148fe85-6193-421c-fd1f-0bb708bc4c62","trusted":true,"execution":{"iopub.status.busy":"2024-11-22T13:06:05.549312Z","iopub.execute_input":"2024-11-22T13:06:05.550105Z","iopub.status.idle":"2024-11-22T13:06:06.08277Z","shell.execute_reply.started":"2024-11-22T13:06:05.550069Z","shell.execute_reply":"2024-11-22T13:06:06.081881Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Visualising Few Sample Images","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport torchvision.transforms.v2 as v2\n\ntest_transform = v2.Compose([\n    v2.Grayscale(num_output_channels=3),\n    v2.Resize((224, 224)),\n    v2.RandomHorizontalFlip(),\n    v2.RandomRotation(10),\n    v2.RandomAdjustSharpness(sharpness_factor=5, p=0.5),\n    v2.RandomAutocontrast(p=0.5),\n    v2.ToImage(),\n])\nsample_imgs = np.random.randint(low=0, high=len(train_df),size=5)\nfig, axs = plt.subplots(nrows=5, ncols=2, figsize=(8,10))\n\n\nfor i in range(5):    \n        img_path, img_class, _ = train_df.iloc[sample_imgs[i]]\n        img = Image.open(img_path).convert('RGB')\n        img_transformed = test_transform(img).permute(1,2,0).numpy()\n        \n        axs[i,0].imshow(img)\n        axs[i,0].set_title(f'original: image class = {img_class}')\n        \n        axs[i,0].axis('off')\n\n        axs[i,1].imshow(img_transformed)\n        axs[i,1].set_title(f'transformed: image class = {img_class}')\n        axs[i,1].axis('off')\n\nfig.suptitle('Visualising the sample images')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T13:07:57.519241Z","iopub.execute_input":"2024-11-22T13:07:57.519773Z","iopub.status.idle":"2024-11-22T13:07:58.236203Z","shell.execute_reply.started":"2024-11-22T13:07:57.51974Z","shell.execute_reply":"2024-11-22T13:07:58.235326Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Creating the Dataset\n\nCreating the pytorch custom dataset and respective dataloaders","metadata":{}},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom PIL import Image\nfrom torch.utils.data import Dataset\nimport torchvision.transforms.v2 as v2\n\ntorch.manual_seed(42)\ntorch.cuda.manual_seed(42)\nclass TumorDataset(Dataset):\n    def __init__(self, dataframe, transform=None):\n        self.dataframe = dataframe\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        img_path = self.dataframe.iloc[idx]['image_path']\n        img_class = self.dataframe.iloc[idx]['encoded_class']\n\n        image = Image.open(img_path)\n        image = image.convert('RGB')\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, img_class\n\n\ntransform = v2.Compose([\n    v2.Grayscale(num_output_channels=3),\n    v2.Resize((224, 224)),\n    v2.RandomHorizontalFlip(),\n    v2.RandomRotation(10),\n    v2.RandomAdjustSharpness(sharpness_factor=5, p=0.5),\n    v2.RandomAutocontrast(p=0.5),\n    v2.ToImage(),\n    v2.ToDtype(torch.float32, scale=True),\n    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\ntrain_dataset = TumorDataset(train_df, transform=transform)\ntest_dataset = TumorDataset(test_df, transform=transform)","metadata":{"id":"T_bjPPRpicdf","trusted":true,"execution":{"iopub.status.busy":"2024-11-22T13:08:56.55337Z","iopub.execute_input":"2024-11-22T13:08:56.553988Z","iopub.status.idle":"2024-11-22T13:08:56.56348Z","shell.execute_reply.started":"2024-11-22T13:08:56.553954Z","shell.execute_reply":"2024-11-22T13:08:56.562739Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\n# set torch random seed\n\n# Create data loaders for training and testing datasets\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)","metadata":{"id":"oOA-w9ZPoV2h","trusted":true,"execution":{"iopub.status.busy":"2024-11-22T13:09:00.563357Z","iopub.execute_input":"2024-11-22T13:09:00.563653Z","iopub.status.idle":"2024-11-22T13:09:00.568621Z","shell.execute_reply.started":"2024-11-22T13:09:00.563627Z","shell.execute_reply":"2024-11-22T13:09:00.567516Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"markdown","source":"\n### Machine Learning Models\n\n1. SVM\n2. Random Forest\n3. XGBoost\n\n\n**Extracting Feature Vector from ResNet 18 model**\n","metadata":{"id":"VRa9xb2VqbeS"}},{"cell_type":"code","source":"import torch.nn as nn\nimport torchvision.models as models\n\n\n# Load pre-trained ResNet18 model\nresnet18 = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n\n# Remove the final classification layer\nnum_ftrs = resnet18.fc.in_features\nresnet18.fc = nn.Identity()\n\n\n\n# Move the model to the appropriate device (CPU or GPU)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nresnet18 = resnet18.to(device)\n\n# Extract feature vectors for training data\ntrain_features = []\ntrain_labels = []\nresnet18.eval()  # Set model to evaluation mode\nwith torch.no_grad():\n  for images, labels in train_loader:\n    images = images.to(device)\n    features = resnet18(images)\n    train_features.extend(features.cpu().numpy())\n    train_labels.extend(labels)\n\n# Extract feature vectors for testing data\ntest_features = []\ntest_labels = []\nwith torch.no_grad():\n  for images, labels in test_loader:\n    images = images.to(device)\n    features = resnet18(images)\n    test_features.extend(features.cpu().numpy())\n    test_labels.extend(labels)\n\n# Convert lists to numpy arrays\ntrain_features = np.array(train_features)\ntrain_labels = np.array(train_labels)\ntest_features = np.array(test_features)\ntest_labels = np.array(test_labels)","metadata":{"id":"vO7sJf-llErx","trusted":true,"execution":{"iopub.status.busy":"2024-11-22T15:38:59.00031Z","iopub.execute_input":"2024-11-22T15:38:59.001006Z","iopub.status.idle":"2024-11-22T15:40:00.613372Z","shell.execute_reply.started":"2024-11-22T15:38:59.000971Z","shell.execute_reply":"2024-11-22T15:40:00.612539Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nimport joblib\n\nclassification_report = []\n\n# Train SVM classifier\nsvm_classifier = SVC(kernel='linear')\nsvm_classifier.fit(train_features, train_labels)\nsvm_predictions = svm_classifier.predict(test_features)\nsvm_accuracy = accuracy_score(test_labels, svm_predictions)\nsvm_precision = precision_score(test_labels, svm_predictions, average='weighted')\nsvm_recall = recall_score(test_labels, svm_predictions, average='weighted')\nsvm_f1 = f1_score(test_labels, svm_predictions, average='weighted')\nclassification_report.append({'model': 'SVM', 'accuracy': svm_accuracy, 'precision': svm_precision, 'recall': svm_recall, 'f1': svm_f1})\n\n# Train Random Forest classifier\nrf_classifier = RandomForestClassifier(n_estimators=100)\nrf_classifier.fit(train_features, train_labels)\nrf_predictions = rf_classifier.predict(test_features)\nrf_accuracy = accuracy_score(test_labels, rf_predictions)\nrf_precision = precision_score(test_labels, rf_predictions, average='weighted')\nrf_recall = recall_score(test_labels, rf_predictions, average='weighted')\nrf_f1 = f1_score(test_labels, rf_predictions, average='weighted')\nclassification_report.append({'model': 'Random Forest', 'accuracy': rf_accuracy, 'precision': rf_precision, 'recall': rf_recall, 'f1': rf_f1})\n\n# Train XGBoost classifier\nxgb_classifier = XGBClassifier()\nxgb_classifier.fit(train_features, train_labels)\nxgb_predictions = xgb_classifier.predict(test_features)\nxgb_accuracy = accuracy_score(test_labels, xgb_predictions)\nxgb_precision = precision_score(test_labels, xgb_predictions, average='weighted')\nxgb_recall = recall_score(test_labels, xgb_predictions, average='weighted')\nxgb_f1 = f1_score(test_labels, xgb_predictions, average='weighted')\nclassification_report.append({'model': 'XGBoost', 'accuracy': xgb_accuracy, 'precision': xgb_precision, 'recall': xgb_recall, 'f1': xgb_f1})\n\njoblib.dump(svm_classifier, \"svm_classifier.joblib\")\njoblib.dump(rf_classifier, \"rf_classifier.joblib\")\njoblib.dump(xgb_classifier, \"xgb_classifier.joblib\")\n\nclf_df = pd.DataFrame(classification_report)\nclf_df.to_csv('clf_report.csv')\nclf_df.head()","metadata":{"id":"xdoA5cNXn4Vh","outputId":"e2a34a69-fa0d-4d53-8552-71763f7ff431","trusted":true,"execution":{"iopub.status.busy":"2024-11-22T09:13:14.837193Z","iopub.execute_input":"2024-11-22T09:13:14.837669Z","iopub.status.idle":"2024-11-22T09:13:50.245883Z","shell.execute_reply.started":"2024-11-22T09:13:14.83763Z","shell.execute_reply":"2024-11-22T09:13:50.244947Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Creating a Training Module","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom tqdm import tqdm\nfrom torchmetrics import Accuracy, Precision, Recall, F1Score\nclass TrainingModule:\n    def __init__(self, model: nn.Module, device, train_loader, test_loader):\n        self.model = model\n        self.device = device\n        self.train_loader = train_loader\n        self.test_loader = test_loader\n\n        self.results = []\n\n    def train_step(self, epoch, num_epochs, criterion, optimizer):\n        self.model.train()\n        running_loss = 0.0\n        for images, labels in tqdm(self.train_loader, desc=f'Training: {epoch}/{num_epochs}'):\n            images = images.to(self.device)\n            labels = labels.to(self.device)\n            optimizer.zero_grad()\n            outputs = self.model(images)\n            loss = criterion(outputs, labels)\n            \n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n\n        running_loss = running_loss / len(self.train_loader)\n        return running_loss\n\n    def test_step(self, epoch, num_epochs, criterion):\n        self.model.eval()\n        running_loss = 0.0\n        with torch.no_grad():\n            for images, labels in tqdm(self.test_loader, desc=f'Testing: {epoch}/{num_epochs}'):\n                images = images.to(self.device)\n                labels = labels.to(self.device)\n                outputs = self.model(images)\n                loss = criterion(outputs, labels)\n                running_loss += loss.item()\n\n        running_loss = running_loss / len(self.test_loader)\n        return running_loss\n\n    def train(self, criterion, optimizer, num_epochs, save_step):\n        for epoch in range(1,num_epochs+1):\n            train_loss = self.train_step(epoch, num_epochs, criterion, optimizer)\n            test_loss = self.test_step(epoch, num_epochs, criterion)\n\n            self.results.append({'epoch': epoch, \n                                 'train_loss': train_loss,\n                                 'test_loss': test_loss\n                                })\n            \n            print(f'Epoch {epoch}/{num_epochs} - Train Loss: {train_loss:.4f} - Test Loss: {test_loss:.4f}')\n            if epoch % save_step == 0:\n                self.save_model()\n                self.get_results()\n\n        self.save_model()\n        self.get_results()\n\n    def get_classification_report(self):\n\n        # load the state_dict from model_path,\n        self.model.eval()\n\n        accuracy = Accuracy(task='multiclass', num_classes=len(classnames)).to(self.device)\n        precision = Precision(task='multiclass',average='weighted', num_classes=len(classnames)).to(self.device)\n        recall = Recall(task='multiclass',average='weighted', num_classes=len(classnames)).to(self.device)\n        f1 = F1Score(task='multiclass',average='weighted', num_classes=len(classnames)).to(self.device)\n        with torch.no_grad():\n            for images, labels in tqdm(self.test_loader,\n                                       desc=f'Generating Classification Report'):\n                images = images.to(self.device)\n                labels = labels.to(self.device)\n                outputs = self.model(images)\n                accuracy.update(outputs, labels)\n                precision.update(outputs, labels)\n                recall.update(outputs, labels)\n                f1.update(outputs, labels)\n        \n        return {\n                'model': self.model.__class__.__name__,\n                'accuracy': accuracy.compute().cpu().item(),\n                'precision': precision.compute().cpu().item(),\n                'recall': recall.compute().cpu().item(),\n                'f1': f1.compute().cpu().item()\n                }\n\n\n    def get_results(self):\n        results_df  = pd.DataFrame(self.results)\n        results_df.to_csv(f'{self.model.__class__.__name__}_results.csv',\n                          index=False)\n        return results_df\n\n    def save_model(self):\n        torch.save(self.model.state_dict(), \n                   f'{self.model.__class__.__name__}.pth')\n        ","metadata":{"id":"k5Vr1d4ep54d","trusted":true,"execution":{"iopub.status.busy":"2024-11-22T13:15:02.042356Z","iopub.execute_input":"2024-11-22T13:15:02.042747Z","iopub.status.idle":"2024-11-22T13:15:03.653337Z","shell.execute_reply.started":"2024-11-22T13:15:02.042714Z","shell.execute_reply":"2024-11-22T13:15:03.652447Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Transfer Learning","metadata":{}},{"cell_type":"markdown","source":"#### Transfer Learning with ResNet 18 Model","metadata":{}},{"cell_type":"code","source":"class ResNet18Model(nn.Module):\n    def __init__(self, num_classes):\n        super(ResNet18Model, self).__init__()\n        self.base_model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n        num_ftrs = self.base_model.fc.in_features\n        self.base_model.fc = nn.Sequential(\n            nn.Linear(num_ftrs, num_classes),     \n            nn.Softmax(dim=1)\n        )\n       \n\n    def forward(self, x):\n        return self.base_model(x)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Instantiate the model, loss function, and optimizer\nresnet18_model = ResNet18Model(len(classnames)).to(device)\ncriterion = nn.CrossEntropyLoss().to(device)\noptimizer = torch.optim.Adam(resnet18_model.parameters(), lr=0.0001)\n\n# Move the model to the device\nresnet18_model = resnet18_model.to(device)\n\n# Create a training module\nresnet_training_module = TrainingModule(resnet18_model, device, train_loader, test_loader)\n","metadata":{"id":"FqiQrVSBueFz","trusted":true,"execution":{"iopub.status.busy":"2024-11-22T15:36:14.495379Z","iopub.execute_input":"2024-11-22T15:36:14.496145Z","iopub.status.idle":"2024-11-22T15:36:14.748282Z","shell.execute_reply.started":"2024-11-22T15:36:14.496111Z","shell.execute_reply":"2024-11-22T15:36:14.747331Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"resnet_training_module.train(criterion, optimizer, num_epochs=20, save_step=5)","metadata":{"id":"Ps8oRPH63PIa","trusted":true,"execution":{"iopub.status.busy":"2024-11-22T09:14:34.802101Z","iopub.execute_input":"2024-11-22T09:14:34.802417Z","iopub.status.idle":"2024-11-22T09:37:03.457237Z","shell.execute_reply.started":"2024-11-22T09:14:34.802391Z","shell.execute_reply":"2024-11-22T09:37:03.456298Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Generate classification report\nresnet_results = resnet_training_module.get_results()\nresnet_clf_report = resnet_training_module.get_classification_report()\nclassification_report.append(resnet_clf_report)\nclf_df = pd.DataFrame(classification_report)\nclf_df.head()","metadata":{"id":"SIZQUTRz2Bn7","outputId":"c2efdb75-bcc2-4a73-a65c-655e96615e59","trusted":true,"execution":{"iopub.status.busy":"2024-11-22T09:37:10.613752Z","iopub.execute_input":"2024-11-22T09:37:10.614096Z","iopub.status.idle":"2024-11-22T09:37:20.968264Z","shell.execute_reply.started":"2024-11-22T09:37:10.614065Z","shell.execute_reply":"2024-11-22T09:37:20.967357Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"clf_df.to_csv('clf_report.csv')","metadata":{"id":"5iEMeGTG4d08","outputId":"d694cecf-5548-4712-d58a-8b4813336939","trusted":true,"execution":{"iopub.status.busy":"2024-11-22T09:37:52.513241Z","iopub.execute_input":"2024-11-22T09:37:52.514031Z","iopub.status.idle":"2024-11-22T09:37:52.519179Z","shell.execute_reply.started":"2024-11-22T09:37:52.513997Z","shell.execute_reply":"2024-11-22T09:37:52.518191Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.plot(resnet_results['epoch'],resnet_results['train_loss'], label='train loss')\nplt.plot(resnet_results['epoch'],resnet_results['test_loss'], label='test loss')\nplt.title('Resnet loss curves')\nplt.legend(labels=['train loss', 'test loss'])\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T09:40:01.673895Z","iopub.execute_input":"2024-11-22T09:40:01.674683Z","iopub.status.idle":"2024-11-22T09:40:01.929277Z","shell.execute_reply.started":"2024-11-22T09:40:01.674649Z","shell.execute_reply":"2024-11-22T09:40:01.927612Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Transfer Learning with DenseNet Model","metadata":{}},{"cell_type":"code","source":"class DenseNetModel(nn.Module):\n    \n    def __init__(self, num_classes):\n        super(DenseNetModel, self).__init__()\n        # Load pre-trained DenseNet121 model\n        self.base_model = models.densenet121(weights=models.DenseNet121_Weights.DEFAULT)\n\n        # Replace the classifier with a new one for our number of classes\n        num_ftrs = self.base_model.classifier.in_features\n        self.base_model.classifier = nn.Sequential(\n            nn.Linear(num_ftrs, num_classes),\n            nn.Softmax(dim=1)\n        )\n\n    def forward(self, x):\n        x = self.base_model(x)\n        return x\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Instantiate the model, loss function, and optimizer\ndensenet_model = DenseNetModel(len(classnames)).to(device)\ncriterion = nn.CrossEntropyLoss().to(device)\noptimizer = torch.optim.Adam(densenet_model.parameters(), lr=0.001)\n\n# Move the model to the device\ndensenet_model = densenet_model.to(device)\n\n# Create a training module\ndensenet_training_module = TrainingModule(densenet_model, device, train_loader, test_loader)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T15:36:40.934441Z","iopub.execute_input":"2024-11-22T15:36:40.935328Z","iopub.status.idle":"2024-11-22T15:36:41.385622Z","shell.execute_reply.started":"2024-11-22T15:36:40.93529Z","shell.execute_reply":"2024-11-22T15:36:41.384661Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"densenet_training_module.train(criterion, optimizer, num_epochs=20, save_step=5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T13:15:23.493603Z","iopub.execute_input":"2024-11-22T13:15:23.494464Z","iopub.status.idle":"2024-11-22T13:55:17.777911Z","shell.execute_reply.started":"2024-11-22T13:15:23.494427Z","shell.execute_reply":"2024-11-22T13:55:17.776963Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Generate classification report\ndensenet_results = densenet_training_module.get_results()\nclassification_report_new = densenet_training_module.get_classification_report()\nclassification_report.append(classification_report_new)\nclf_df = pd.DataFrame(classification_report)\nclf_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T13:58:48.804814Z","iopub.execute_input":"2024-11-22T13:58:48.805664Z","iopub.status.idle":"2024-11-22T13:59:05.045432Z","shell.execute_reply.started":"2024-11-22T13:58:48.805615Z","shell.execute_reply":"2024-11-22T13:59:05.044673Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"clf_df.to_csv('clf_report.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T14:00:03.560072Z","iopub.execute_input":"2024-11-22T14:00:03.5607Z","iopub.status.idle":"2024-11-22T14:00:03.566386Z","shell.execute_reply.started":"2024-11-22T14:00:03.560646Z","shell.execute_reply":"2024-11-22T14:00:03.56542Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.plot(densenet_results['epoch'],densenet_results['train_loss'], label='train loss')\nplt.plot(densenet_results['epoch'],densenet_results['test_loss'], label='test loss')\nplt.title('DenseNet loss curves')\nplt.legend(labels=['train loss', 'test loss'])\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T14:00:46.804012Z","iopub.execute_input":"2024-11-22T14:00:46.804624Z","iopub.status.idle":"2024-11-22T14:00:47.095298Z","shell.execute_reply.started":"2024-11-22T14:00:46.804588Z","shell.execute_reply":"2024-11-22T14:00:47.094513Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Transfer Learning with EfficentNetB0 Model","metadata":{}},{"cell_type":"code","source":"class EFNetModel(nn.Module):\n    def __init__(self, num_classes):\n        super(EFNetModel, self).__init__()\n        self.base_model = models.efficientnet_b0(\n            weights=models.EfficientNet_B0_Weights.DEFAULT\n        )\n        num_ftrs = self.base_model.classifier[1].in_features\n        self.base_model.classifier[1] = nn.Sequential(\n            nn.Linear(self.base_model.classifier[1].in_features, num_classes),\n            nn.Softmax(dim=1)\n        )\n    \n    def forward(self, x):\n        return self.base_model(x)\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Instantiate the model, loss function, and optimizer\nefnet_model = EFNetModel(len(classnames)).to(device)\ncriterion = nn.CrossEntropyLoss().to(device)\noptimizer = torch.optim.Adam(efnet_model.parameters(), lr=0.001)\n\n# Move the model to the device\nefnet_model = efnet_model.to(device)\n\n# Create a training module\nefnet_training_module = TrainingModule(efnet_model,\n                                            device, train_loader, test_loader)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T15:36:27.040916Z","iopub.execute_input":"2024-11-22T15:36:27.041288Z","iopub.status.idle":"2024-11-22T15:36:27.471931Z","shell.execute_reply.started":"2024-11-22T15:36:27.041254Z","shell.execute_reply":"2024-11-22T15:36:27.470857Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"efnet_training_module.train(criterion, optimizer, num_epochs=10, save_step=5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T10:32:39.830831Z","iopub.execute_input":"2024-11-22T10:32:39.831644Z","iopub.status.idle":"2024-11-22T10:45:49.665357Z","shell.execute_reply.started":"2024-11-22T10:32:39.831612Z","shell.execute_reply":"2024-11-22T10:45:49.664256Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Generate classification report\nefnet_results = efnet_training_module.get_results()\nef_clf_report = efnet_training_module.get_classification_report()\nclassification_report.append(ef_clf_report)\nclf_df = pd.DataFrame(classification_report)\nclf_df.tail()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T10:45:54.740767Z","iopub.execute_input":"2024-11-22T10:45:54.74149Z","iopub.status.idle":"2024-11-22T10:46:05.050088Z","shell.execute_reply.started":"2024-11-22T10:45:54.74145Z","shell.execute_reply":"2024-11-22T10:46:05.049167Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"clf_df.to_csv('clf_report.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T07:09:15.757887Z","iopub.execute_input":"2024-11-21T07:09:15.758268Z","iopub.status.idle":"2024-11-21T07:09:15.763928Z","shell.execute_reply.started":"2024-11-21T07:09:15.758235Z","shell.execute_reply":"2024-11-21T07:09:15.762933Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.plot(efnet_results['epoch'],efnet_results['train_loss'], label='train loss')\nplt.plot(efnet_results['epoch'],efnet_results['test_loss'], label='test loss')\nplt.title('EFNet loss curves')\nplt.legend(labels=['train loss', 'test loss'])\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T10:48:31.61126Z","iopub.execute_input":"2024-11-22T10:48:31.611927Z","iopub.status.idle":"2024-11-22T10:48:31.787001Z","shell.execute_reply.started":"2024-11-22T10:48:31.611894Z","shell.execute_reply":"2024-11-22T10:48:31.786178Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Deep Learning\n\nDeep Learning using convolutional layers","metadata":{}},{"cell_type":"code","source":"import torch.nn.functional as F\n\nclass BTModel(nn.Module):\n    def __init__(self, num_classes):\n        super(BTModel, self).__init__()\n\n        self.features = nn.Sequential(\n           nn.Conv2d(3, 16, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            \n        )\n        \n        self.classifier = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(64 * 28 * 28, 128),\n            nn.ReLU(inplace=True),\n            nn.Linear(128, num_classes),\n            nn.Softmax(dim=1)\n        )\n        \n    \n    def forward(self, x):\n        x = self.features(x)\n        x = self.classifier(x)\n        return x\n\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Instantiate the model, loss function, and optimizer\nbt_model = BTModel(len(classnames)).to(device)\ncriterion = nn.CrossEntropyLoss().to(device)\noptimizer = torch.optim.Adam(bt_model.parameters(), lr=0.001)\n\n# Move the model to the device\nbt_model = bt_model.to(device)\n\n# Create a training module\nbt_training_module = TrainingModule(bt_model, device, train_loader, test_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T14:36:11.913839Z","iopub.execute_input":"2024-11-22T14:36:11.914468Z","iopub.status.idle":"2024-11-22T14:36:11.969985Z","shell.execute_reply.started":"2024-11-22T14:36:11.914431Z","shell.execute_reply":"2024-11-22T14:36:11.969123Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"bt_training_module.train(criterion, optimizer, num_epochs=20, save_step=5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T15:02:24.695326Z","iopub.execute_input":"2024-11-22T15:02:24.695674Z","iopub.status.idle":"2024-11-22T15:22:32.161732Z","shell.execute_reply.started":"2024-11-22T15:02:24.69564Z","shell.execute_reply":"2024-11-22T15:22:32.160882Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Generate classification report\nbt_results = bt_training_module.get_results()\nbt_clf_report = bt_training_module.get_classification_report()\nclassification_report = []\nclassification_report.append(bt_clf_report)\nclf_df = pd.DataFrame(classification_report)\nclf_df.tail()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T15:50:26.465005Z","iopub.execute_input":"2024-11-22T15:50:26.465348Z","iopub.status.idle":"2024-11-22T15:50:36.712554Z","shell.execute_reply.started":"2024-11-22T15:50:26.465316Z","shell.execute_reply":"2024-11-22T15:50:36.711632Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"clf_df.to_csv('clf_report.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T15:29:32.413824Z","iopub.execute_input":"2024-11-22T15:29:32.414181Z","iopub.status.idle":"2024-11-22T15:29:32.42011Z","shell.execute_reply.started":"2024-11-22T15:29:32.414149Z","shell.execute_reply":"2024-11-22T15:29:32.419094Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.plot(bt_results['epoch'],bt_results['train_loss'], label='train loss')\nplt.plot(bt_results['epoch'],bt_results['test_loss'], label='test loss')\nplt.title('BT loss curves')\nplt.legend(labels=['train loss', 'test loss'])\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T15:33:36.684564Z","iopub.execute_input":"2024-11-22T15:33:36.685269Z","iopub.status.idle":"2024-11-22T15:33:36.950169Z","shell.execute_reply.started":"2024-11-22T15:33:36.685232Z","shell.execute_reply":"2024-11-22T15:33:36.949316Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Prediction","metadata":{}},{"cell_type":"code","source":"import joblib\nimport torch\n\n\nlabel_encoder = joblib.load('label_encoder.joblib')\nrf_classifier = joblib.load('rf_classifier.joblib')\nsvm_classifier = joblib.load('svm_classifier.joblib')\nxgb_classifier = joblib.load('xgb_classifier.joblib')\n\nresnet_model = ResNet18Model(len(classnames)).to(device)\nresnet_model.load_state_dict(torch.load(\"./ResNet18Model.pth\",\n                                            map_location=device,\n                                            weights_only=False))\n\ndensenet_model = DenseNetModel(len(classnames)).to(device)\ndensenet_model.load_state_dict(torch.load(\"./DenseNetModel.pth\",\n                                         map_location=device,\n                                         weights_only=False))\n\nefnet_model = EFNetModel(len(classnames)).to(device)\nefnet_model.load_state_dict(torch.load(\"./EFNetModel.pth\",\n                                           map_location=device,\n                                           weights_only=False))\n\nbt_model = BTModel(len(classnames)).to(device)\nbt_model.load_state_dict(torch.load(\"./BTModel.pth\",\n                                        map_location=device,\n                                        weights_only=False))\n\ndef predict(image):\n    densenet_model.eval()\n    efnet_model.eval()\n    bt_model.eval()\n    resnet_model.eval()\n    \n    with torch.no_grad():\n        image_tensor = image.unsqueeze(0).to(device)\n        resnet_class = torch.argmax(resnet_model(image_tensor))\n        densenet_class = torch.argmax(densenet_model(image_tensor))\n        efnet_class = torch.argmax(efnet_model(image_tensor))\n        bt_class = torch.argmax(bt_model(image_tensor))\n        \n        image_numpy = resnet18(image_tensor)\n        image_numpy = image_numpy.cpu().numpy()\n    \n    rf_class = rf_classifier.predict(image_numpy)\n    svm_class = svm_classifier.predict(image_numpy)\n    xgb_class = xgb_classifier.predict(image_numpy)\n\n    results = {\n        'resnet': resnet_class.item(),\n        'densenet': densenet_class.item(),\n        'efnet': efnet_class.item(),\n        'bt': bt_class.item(),\n        'rf': rf_class[0],\n        'svm': svm_class[0],\n        'xgb': xgb_class[0]        \n    }\n\n    predicted_classes = np.array(list(results.values()))\n    unique_values, counts = np.unique(predicted_classes, return_counts=True)\n\n    # # Find the index of the maximum count\n    max_count_index = np.argmax(counts)\n\n    # # Get the number that appears the most\n    most_frequent_number = unique_values[max_count_index]\n    \n    predicted_class = label_encoder.inverse_transform([most_frequent_number])[0]\n    results['resnet'] = label_encoder.inverse_transform([results['resnet']])[0]\n    results['densenet'] = label_encoder.inverse_transform([results['densenet']])[0]\n    results['efnet'] = label_encoder.inverse_transform([results['efnet']])[0]\n    results['bt'] = label_encoder.inverse_transform([results['bt']])[0]\n    results['rf'] = label_encoder.inverse_transform([results['rf']])[0]\n    results['xgb'] = label_encoder.inverse_transform([results['xgb']])[0]\n    results['svm'] = label_encoder.inverse_transform([results['svm']])[0]\n    results['final'] = predicted_class\n    \n    return predicted_class, most_frequent_number, results\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T15:38:49.197244Z","iopub.execute_input":"2024-11-22T15:38:49.198237Z","iopub.status.idle":"2024-11-22T15:38:50.251716Z","shell.execute_reply.started":"2024-11-22T15:38:49.198199Z","shell.execute_reply":"2024-11-22T15:38:50.250655Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import random\nimport numpy as np\nfrom PIL import Image\n\nsample_imgs = np.random.randint(low=0, high=len(train_df),size=10)\nfig, axs = plt.subplots(nrows=5, ncols=2, figsize=(10,10))\nk = 0\nfor i in range(5):\n    for j in range(2):\n        img_path, img_class, _ = train_df.iloc[sample_imgs[k]]\n        img = Image.open(img_path).convert('RGB')\n        img_transformed = transform(img)\n        predicted_class,_,results = predict(img_transformed)\n        axs[i,j].imshow(img)\n        axs[i,j].set_title(f'original={img_class}, predicted={predicted_class}')\n        axs[i,j].axis('off')\n        k+=1\n        \nfig.suptitle('Visualising the sample images')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T15:40:00.615231Z","iopub.execute_input":"2024-11-22T15:40:00.616113Z","iopub.status.idle":"2024-11-22T15:40:01.984008Z","shell.execute_reply.started":"2024-11-22T15:40:00.616074Z","shell.execute_reply":"2024-11-22T15:40:01.983112Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\ndef generate_classification_report():\n    accuracy = Accuracy(task='multiclass', num_classes=len(classnames)).to(device)\n    precision = Precision(task='multiclass',average='weighted', num_classes=len(classnames)).to(device)\n    recall = Recall(task='multiclass',average='weighted', num_classes=len(classnames)).to(device)\n    f1 = F1Score(task='multiclass',average='weighted', num_classes=len(classnames)).to(device)\n\n    for images, labels in tqdm(test_loader, desc='Generating classification report'):\n        outputs = []\n        images = images.to(device)\n        labels = labels.to(device)\n        for image in images:\n            \n            outputs.append(int(predict(image)[1]))\n\n        outputs = torch.tensor(outputs).to(device)\n\n        accuracy.update(outputs, labels)\n        precision.update(outputs, labels)\n        recall.update(outputs, labels)\n        f1.update(outputs, labels)\n\n    return {\n            'model': \"final\",\n            'accuracy': accuracy.compute().cpu().item(),\n            'precision': precision.compute().cpu().item(),\n            'recall': recall.compute().cpu().item(),\n            'f1': f1.compute().cpu().item()\n        }\n\n\nfinal_clf_report = generate_classification_report()\nclassification_report.append(final_clf_report)\nclf_df = pd.DataFrame(classification_report)\nclf_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T15:41:09.840745Z","iopub.execute_input":"2024-11-22T15:41:09.84112Z","iopub.status.idle":"2024-11-22T15:42:12.91556Z","shell.execute_reply.started":"2024-11-22T15:41:09.84109Z","shell.execute_reply":"2024-11-22T15:42:12.914625Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"clf_report","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T15:53:04.994193Z","iopub.execute_input":"2024-11-22T15:53:04.99502Z","iopub.status.idle":"2024-11-22T15:53:05.005829Z","shell.execute_reply.started":"2024-11-22T15:53:04.994985Z","shell.execute_reply":"2024-11-22T15:53:05.004988Z"}},"outputs":[],"execution_count":null}]}